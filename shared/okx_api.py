# shared/okx_api.py
import os
import json
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Optional
from dateutil import tz
import logging
from pathlib import Path
from dotenv import load_dotenv
import pandas as pd
import redis
import numpy as np

# === –ó–∞–≥—Ä—É–∂–∞–µ–º .env –∏–∑ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞ ===
env_path = Path(__file__).parent.parent / ".env"
if env_path.exists():
    load_dotenv(dotenv_path=env_path)
    print(f"‚úÖ .env –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {env_path}")
else:
    print(f"‚ùå .env –Ω–µ –Ω–∞–π–¥–µ–Ω: {env_path}")

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —á–∞—Å–æ–≤—ã—Ö –ø–æ—è—Å–æ–≤ ===
MSK_TZ = tz.gettz("Europe/Moscow")
UTC_TZ = timezone.utc

# === –õ–æ–≥–≥–µ—Ä ===
logger = logging.getLogger("okx_fetcher")
logger.setLevel(logging.INFO)

def setup_logging():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∞–∫—Ç—É–∞–ª—å–Ω—ã–º —Ñ–∞–π–ª–æ–º –Ω–∞ —Å–µ–≥–æ–¥–Ω—è"""
    log_dir = Path(__file__).parent.parent / "data" / "logs"
    log_dir.mkdir(exist_ok=True)
    log_file = log_dir / f"fetching_{datetime.now().strftime('%Y-%m-%d')}.log"
    
    formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # –§–∞–π–ª–æ–≤—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
    file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    
    # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

# === API-–∫–ª—é—á–∏ ===
API_KEY = os.getenv("OKX_API_KEY")
SECRET_KEY = os.getenv("OKX_API_SECRET")
PASSPHRASE = os.getenv("OKX_PASSPHRASE")

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ OKX ===
from okx import OkxRestClient
client = OkxRestClient(API_KEY, SECRET_KEY, PASSPHRASE)

# === –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Redis ===
try:
    redis_client = redis.Redis(host='okx-redis', port=6379, db=0, decode_responses=False)
    redis_client.ping()
    logger.info("‚úÖ –£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Redis")
except redis.ConnectionError:
    logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Redis")
    redis_client = None

# === –ü—É—Ç—å –∫ Parquet ===
PROCESSED_DIR = Path(__file__).parent.parent / "data" / "processed"
os.makedirs(PROCESSED_DIR, exist_ok=True)

def align_timestamp_to_candle(timestamp_ms: int, bar: str) -> int:
    """–í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç timestamp –ø–æ —Å–µ—Ç–∫–µ —Å–≤–µ—á–µ–π"""
    bar_to_seconds = {
        "1m": 60, "3m": 180, "5m": 300,
        "15m": 900, "30m": 1800, "1h": 3600
    }
    period = bar_to_seconds.get(bar, 180)
    ts_sec = timestamp_ms // 1000
    aligned_sec = (ts_sec // period) * period
    return aligned_sec * 1000

def save_to_parquet(candles: List[Dict], pair: str, bar: str):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–≤–µ—á–∏ –≤ Parquet —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
    filename = PROCESSED_DIR / f"{pair.lower().replace('-', '_')}_{bar}.parquet"
    
    try:
        # –°–æ–∑–¥–∞–µ–º DataFrame –∏–∑ –Ω–æ–≤—ã—Ö —Å–≤–µ—á–µ–π
        new_df = pd.DataFrame(candles)
        
        if filename.exists():
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            existing_df = pd.read_parquet(filename)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            combined_df = pd.concat([existing_df, new_df])
            combined_df = combined_df.drop_duplicates(subset=["timestamp_ms"], keep="last")
        else:
            combined_df = new_df
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        combined_df = combined_df.sort_values("timestamp_ms", ascending=True)
        combined_df.to_parquet(filename, index=False)
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(combined_df)} —Å–≤–µ—á–µ–π –≤ {filename}")
        return combined_df
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ Parquet: {e}")
        return pd.DataFrame()

def load_from_parquet(pair: str, bar: str) -> List[Dict]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–≤–µ—á–∏ –∏–∑ Parquet"""
    filename = PROCESSED_DIR / f"{pair.lower().replace('-', '_')}_{bar}.parquet"
    if not filename.exists():
        return []
    try:
        df = pd.read_parquet(filename)
        return df.to_dict("records")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è Parquet: {e}")
        return []

def save_to_redis(pair: str, bar: str, candles: List[Dict]):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ Redis"""
    if not redis_client:
        logger.warning("Redis –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ")
        return
    
    key = f"{pair}:{bar}"
    try:
        pipe = redis_client.pipeline()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å–≤–µ—á–∏
        for candle in candles:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º timestamp_ms –∫–∞–∫ score
            pipe.zadd(key, {json.dumps(candle): candle["timestamp_ms"]})
        
        # –£–¥–∞–ª—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –¥–∞–Ω–Ω—ã–µ (—Å—Ç–∞—Ä—à–µ 7 –¥–Ω–µ–π)
        seven_days_ago = int((datetime.now() - timedelta(days=7)).timestamp() * 1000)
        pipe.zremrangebyscore(key, 0, seven_days_ago)
        
        pipe.execute()
        logger.info(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ Redis: {len(candles)} —Å–≤–µ—á–µ–π –≤ {key}")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è Redis: {e}")

def load_from_redis(pair: str, bar: str, start_ms: int = None, end_ms: int = None) -> List[Dict]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–≤–µ—á–∏ –∏–∑ Redis –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ"""
    if not redis_client:
        return []
    
    key = f"{pair}:{bar}"
    if not redis_client.exists(key):
        return []
    
    try:
        if start_ms is None:
            start_ms = 0
        if end_ms is None:
            end_ms = "+inf"
        
        data = redis_client.zrangebyscore(key, start_ms, end_ms)
        return [json.loads(d) for d in data]
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∏–∑ Redis: {e}")
        return []

def get_btc_usdt_candles(
    start_dt: datetime,
    end_dt: datetime,
    bar: str = "3m",
    max_limit: int = 100,
    force_fetch: bool = False
) -> Optional[List[Dict]]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–≤–µ—á–∏ BTC/USDT –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞–≥–∏–Ω–∞—Ü–∏—é –ø–æ `before` –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏.
    """
    setup_logging()

    def to_utc_ms(dt: datetime):
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=MSK_TZ)
        return int(dt.astimezone(UTC_TZ).timestamp() * 1000)

    start_ms = to_utc_ms(start_dt)
    end_ms = to_utc_ms(end_dt) - 1  # –ó–∞—â–∏—Ç–∞ –æ—Ç –±—É–¥—É—â–µ–≥–æ
    end_ms = align_timestamp_to_candle(end_ms, bar)

    if start_ms >= end_ms:
        logger.warning(f"‚õî –ù–µ–≤–µ—Ä–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω: start_ms >= end_ms ({start_ms} >= {end_ms})")
        return None

    pair = "BTC-USDT"

    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ Redis
    if not force_fetch:
        cached = load_from_redis(pair, bar, start_ms, end_ms)
        if cached:
            logger.info(f"‚ôªÔ∏è –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ –∫—ç—à–∞ Redis: {len(cached)} —Å–≤–µ—á–µ–π")
            return cached

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ Parquet
        all_candles = load_from_parquet(pair, bar)
        df = pd.DataFrame(all_candles) if all_candles else pd.DataFrame()
        if not df.empty and 'timestamp_ms' in df.columns:
            mask = (df["timestamp_ms"] >= start_ms) & (df["timestamp_ms"] <= end_ms)
            if mask.any():
                result = df[mask].to_dict("records")
                save_to_redis(pair, bar, result)
                logger.info(f"‚ôªÔ∏è –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ –∫—ç—à–∞ Parquet: {len(result)} —Å–≤–µ—á–µ–π")
                return result

    # 3. –ó–∞–ø—Ä–æ—Å –∫ OKX —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π
    all_new_candles = []
    current_end = end_ms
    attempt = 0
    max_attempts = 100  # –ú–∞–∫—Å–∏–º—É–º 100 –∑–∞–ø—Ä–æ—Å–æ–≤

    try:
        while attempt < max_attempts and current_end > start_ms:
            params = {
                "instId": pair,
                "bar": bar,
                "before": str(current_end),
                "limit": str(max_limit)
            }

            logger.debug(f"üì° –ó–∞–ø—Ä–æ—Å #{attempt+1}: before={current_end}")

            response = client.public.get_history_candlesticks(**params)
            if response.get("code") != "0":
                msg = response.get("msg", "Unknown error")
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ API: {msg}")
                break

            candles_data = response.get("data", [])
            if not candles_data:
                logger.info("‚ÑπÔ∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ –æ—Ç–≤–µ—Ç–µ")
                break

            batch = []
            min_ts = None  # –î–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫—É—Ä—Å–æ—Ä–∞

            for c in candles_data:
                ts = int(c[0])
                if ts < start_ms:
                    continue
                dt_utc = datetime.fromtimestamp(ts / 1000, tz=UTC_TZ)
                dt_local = dt_utc.astimezone(MSK_TZ)
                batch.append({
                    "timestamp_ms": ts,
                    "time_utc": dt_utc.strftime("%Y-%m-%d %H:%M:%S"),
                    "time_local": dt_local.strftime("%Y-%m-%d %H:%M:%S"),
                    "open": float(c[1]),
                    "high": float(c[2]),
                    "low": float(c[3]),
                    "close": float(c[4]),
                    "volume_btc": float(c[5]),
                    "volume_usdt": float(c[6]),
                })
                if min_ts is None or ts < min_ts:
                    min_ts = ts

            if not batch:
                logger.info("‚ÑπÔ∏è –ù–µ—Ç –Ω–æ–≤—ã—Ö —Å–≤–µ—á–µ–π –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ")
                break

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –Ω–∞—á–∞–ª–æ (–æ—Ç —Å—Ç–∞—Ä—ã—Ö –∫ –Ω–æ–≤—ã–º)
            batch.reverse()
            all_new_candles = batch + all_new_candles

            # –û–±–Ω–æ–≤–ª—è–µ–º –∫—É—Ä—Å–æ—Ä: –Ω–∞ 1 ms –º–µ–Ω—å—à–µ —Å–∞–º–æ–π —Å—Ç–∞—Ä–æ–π —Å–≤–µ—á–∏
            current_end = min_ts - 1
            current_end = align_timestamp_to_candle(current_end, bar)

            attempt += 1

        if all_new_candles:
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            seen = set()
            unique_candles = []
            for c in all_new_candles:
                if c["timestamp_ms"] not in seen:
                    seen.add(c["timestamp_ms"])
                    unique_candles.append(c)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            save_to_parquet(unique_candles, pair, bar)
            save_to_redis(pair, bar, unique_candles)
            logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(unique_candles)} —Å–≤–µ—á–µ–π")
            return unique_candles

        else:
            logger.warning("‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –Ω–µ –ø–æ–ª—É—á–µ–Ω—ã")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {str(e)}")

    return None